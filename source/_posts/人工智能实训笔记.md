---
title: 人工智能实训笔记
date: 2020-09-07 20:30:58
tags: 
- AI
- Machine Learning
- lecture notes
mathjax: true
---

# Introduction

学校人工智能实训课的笔记。

以天数为主章节，记录一下每天学习的内容，同时作复习之用。

<!--more-->

# Day 1

## numpy

numpy是python语言中做科学计算的基础库。重在数值计算，也是大部分python科学计算库的基础，多用于在大型、多维数组上执行的数值计算。

可以将numpy看作一个容器，用以储存**数值型**数据，并且可以对数值型的数据进行很多不同形式的操作和运算。简单地来看，可以将numpy视为一个多维数组的一种高级表示。

### numpy数组和列表的区别

- numpy数组中只能存放相同类型的数组元素
- numpy数组数据类型的优先级：str > float > int

### numpy数组常见的创建方式

- `np.array()`
- `np.linspace()`
- `np.random.xx()`
- `plt.imread()`。这种方式将一个图片转化为numpy数组

### numpy的常用属性

- `arr.shape`。返回数组的形状
- `arr.ndim`。返回数组的维度
- `arr.size`。返回的数组的元素个数
- `arr.dtype`。返回数组元素的数据类型 

### 数组的索引和切片方式

#### 索引

- `arr[index]`。取行
- `arr[index, col]`。取元素

#### 切片

- `arr[index1 : index2]`。切行
- `arr[:, col1 : col2]`。切列
- `arr[: : -1]`。行倒置
- `arr[:, : : -1]`。列倒置

### 变形

`arr.reshape()`

可以将不同维度的数组变形成其他维度的数组。

注意：变形前和变形后的数组容量要保持一致

### 级联

`arr.concatenate((arr, arr), axis = 0 or 1)`

可以将多个数组进行横向或纵向的拼接

- axis = 0，列向拼接
- axis = 1，横向拼接

### 一些常用的数值方法

- `arr.mean()`
- `arr.max()`
- `arr.sum()`
- `arr.median()`
- `arr.std()`
- `arr.var()`

### 基于矩阵

`arr.T`。矩阵转置

## pandas

与numpy的区别在于pandas主要用来存储和运算非数值型的数据。

### Series

`Series`是一个类似于一维的数据结构（容器），由以下2部分组成：
- `values`：一组数据
- `index`：相关的数据索引标签

### 创建方法

- 由列表或numpy数组创建
- 由字典创建
  - 字典所有的key组成Series的index，所有的value组成values。

#### 重要方法

- `s.isnull(), s.notnull()`。用来判断元素值是否为空。
- `s.unique()`。对Series的元素进行去重。
- `s.nunique()`。统计去重之后的元素个数。

可以通过`bool`数据索引Series的值。例如，可以使用`s[s.notnull()]`选出s中不为空元素。

### DataFrame

`DataFrame`是一个表格型的数据结构，每一行/列都是可以切成一个`Series`

#### 创建方式

- `DataFrame(value = , index = )`
- 通过字典创建
  - 字典的key作为DataFrame的列索引

#### 索引和切片操作

##### 索引

- `df['col']`。取列
- `df.iloc[index]`。索引取行
- `df.iloc[index, col]`。索引取元素

##### 切片

- `df[index1:index2]`。切行
- `df.iloc[:, col1 : col3]`。切列

#### 时间序列的转换：从字符串object到datetime

注意DataFrame中所有的未表示为datetime的数据都应该将其转化为datetime类型以方便后续的进一步操作。

`pd.to_datetime(df[col])`

设置了datetime类型后，就可通过例如`df['date'].dt.month`访问月份信息，同理可以访问如年份、日期等信息。

#### 设置行索引

`df.set_index(col)`。将col列作为原始数据df的行索引

#### 持久化存储

`df.to_xxx()`。`xxx`表示存储的文件类型。

#### 加载数据

`pd.read_xxx()`。可以将外部文件的数据读取加载到df

#### 对数据重新取样

`df.resample(rule)`。对df的数据重新采样，rule就是取样的条件

#### DataFrame中某一列纵向移动

`df['col'].shift(1 or 1)`。将Series的元素整体上下移动1位。

## 数据清洗

### 清洗空值（缺失值）

空值（丢失数据）有两种，分别是：
- None
- np.nan

2种丢失数据的区别
- 数据分析中的np.nan是float类型，而None是对象类型
- 对象类型的空值不能直接参与运算，而nan可以直接参与运算

#### 将空值对应行数据进行删除

`df.dropna()`。

#### 对空值进行填充

`df.fillna()`

常用的填充方式有以下3种

- 近邻值填充
  - 例如`df.fillna(method = 'bfill', axis = 0)`表示用同一列的后一个数据填充
- 均值，中位数填充
  - 需要使用循环和条件语句进行填充
  - 需要注意的是，若Series中含有空值，则将其作为参数调用`np.median()`时必返回空
- 任意值填充

### 清洗重复值

`df.drop_duplicates()`

将重复的行数据进行删除。

### 清洗异常值

首先要有一个判定异常值的条件，根据条件再进行相关异常值的清洗操作。

# Day 2

## pandas的高级用法

### pandas级联操作

pandas使用`pd.concat()`函数，与`np.concatenate()`类似，但也有不同。不同之处在于不匹配级联的处理结果。

不匹配级联指的是级联维度的索引不一致，如纵向级联时列索引不一致，横向级联时行索引不一致。

`pd.concat()`的连接方式有2种：
- 外连接`outer`，类似于取并集，同时补空
- 内连接`inner`，类似取交集

### pandas合并操作

级联是对表格进行简单的横/纵向拼接，而合并是对数据的合并。pandas的合并函数是`pd.merge()`。两者的区别在于merge需要提供某一共同列来进行合并。

`pd.merge()`进行合并时，会自动根据两者相同`column`名称的那一列作为key进行合并。

### pandas替换操作

替换操作可以同步作用于Series和DataFrame中。

- 单值替换
  - 普通替换：替换所有符合要求的元素。`df.replace(to_replace=old_value, value=new_value)`
  - 按列指定单值替换：`df.replace(to_replace={column_index : old_value}, value=new_value)`
- 多值替换
  - 列表替换：`df.replace(to_replace=[], value=[])`
  - 字典替换：`df.replace(to_replace={old_value : new_value, ...})`

### 映射

通过创建一个映射关系列表，把values元素和一个特定的标签或者字符串绑定。

map是Series的方法，只能被Series调用。

可以向map中传入字典以建立这种联系。

### 运算工具

使用了函数式编程的思想，通过向Series的apply方法或map方法传入一个函数对Series的元素进行计算并将计算结果形成一个新的Series。

### 随机抽样

DataFrame的take()方法可以实现DataFrame的重排，take()方法的两个重要参数分别是`indices`（隐式索引，标示新的排列顺序）和`axis`（重排的索引方向）。

为了实现随机重排，可以向`indices`传入`np.random.permutation()`。`np.random.permutation(n)`可以产生一个从0到n-1的随机排列。

### 数据的分类处理

数据分类处理的核心函数是DataFrame的`groupby()`方法。

- `df.groupby(by=column_index).groups`返回分组的情况。
- `df.groupby(by=column_index1)[column_index2]`返回一个`pandas.core.groupby.generic.SeriesGroupBy`

### 高级数据聚合

使用了`groupby`分组后，可以使用`transform()`和`apply()`提供自定义的函数实现更多的运算。

`apply()`和`transform()`的区别在于`transform()`返回的是经过了映射的结果。

### 数据加载和分词

使用`pd.read_csv(filepath, sep, header)`可以实现更加细致的读取操作，其中：
- `filepath`标识待读取的文件路径
- `sep`标识用于分词的符号，如常见的`-`
- `header=None`将文件中的第一行也作为DataFrame中的数据而非列索引

### 读取数据库中的数据

这里以`sqlite3`包为例进行操作。

读取数据库首先应该获取连接对象。

```python
import sqlite3
conn = sqlite3.connect('./xxx.sqlite')  # 建立数据库连接
```

连接对象建立后，通过`pd.read_sql()`方法读取数据库中的数据。

```python
sql_df = pd.read_sql('select * from sheet', conn)
```

向数据库中写入数据使用的是DataFrame的`to_sql()`方法。

```python
df.to_sql('sheet_name', conn)
```

### 透视表

透视表是一种可以对数据动态排布并且进行分类汇总的表格格式。在pandas中使用DataFrame的`pivot_table()`方法创建。

透视表的优点有：
- 灵活性高，可以随意定制分析需求
- 脉络清晰且易于理解
- 操作性强

透视表`pivot_table`最重要的参数有4个，分别是：
- `index`：分类的汇总条件。每个pivot_table必须有一个index
- `values`：对需要计算的数据进行筛选
- `columns`：对values字段进行分类
- `aggfunc`：设置我们对数据聚合时进行的函数操作

### 交叉表

交叉表是一种用于计算分组的特殊透视图，用于对数据进行汇总。

`pd.crosstab(index, columns)`，其中`index`为分组数据，是交叉表的行索引，`columns`是交叉表的列索引。

# Day 3

## 机器学习简介

机器学习就是从数据中自动分析获得规律，并且利用这种规律对未知数据进行预测。

### 算法模型

机器学习中很重要的一点是建立和求解算法模型，算法模型是一个待解的数学模型。模型的训练过程可以看作用数据对模型进行求解。

算法模型的作用大致可以分为2种：
- 对未知事物进行预测。预测是利用模型对一件事情计算出一个未知的结果
- 对未知事物进行分类。分类是讲一个未知类别的事物归到最合适的类别种

### 样本数据

样本数据中值得关注的有2类，分别是：
- 特征数据。可以看作是模型的自变量
- 标签/目标。可以看作是因变量

#### 样本数据的载体

通常情况下历史数据都不会存储在数据库中，而是存储在文件中，例如`.csv`文件。

数据库存储数据存在以下几个问题：
- 性能瓶颈：数据量极大的数据很难存储和进行高效的读写
- 数据格式不符合机器学习要求的格式

#### 获取样本数据的常见途径

- kaggle数据竞赛平台
- UCI数据集
- sklearn

## 特征工程

特征工程主要包括以下步骤：
- 特征抽取
- 数据特征的预处理
- 特征选择

### 为何需要特征工程？

样本数据可能存在种种问题不适合直接放入算法模型里参与运算或求解的效率不高。为此特征工程的目的就是为了构建一个更合适、更纯净的样本集，让基于这个数据集训练出来的模型具有更好的预测能力。

### 特征工程的常用工具

`sklearn`包。`sklearn`是python中的机器学习工具，包含了很多知名的机器学习算法的实现，包括常用的：
- 分类模型
- 回归模型
- 聚类模型
- 特征工程

### 特征抽取

特征抽取的目的是讲样本中的不是数据形式的信息（如字符串等）转换成数值型的数据以便模型的计算。特征抽取的常见方法是特征值化，即讲非数值型的特征数据转换成数值形式的数据。

#### 字符串的特征值化

`sklearn.feature_extraction.text`包中的`CountVectorizer`类可以很直接的实现英文字符串的特征值化。向`CountVectorizer`类的`.fit_transform()`方法传入字符串的`list`，生成的结果是一个稀疏矩阵（sparse matrix），稀疏矩阵的结果不太直观，可以调用结果的`.toarray()`方法讲稀疏矩阵转为`list`。

中文的字符串不能直接使用这种方法进行特征值化。`CountVectorizer`类的分词基于英文的句子中的空格和标点符号等，但这种模式对中文分词显然不适用，为此，对于中文字符串，首先需要使用`jieba`包进行分词操作，讲分词后的结果用空格或标点符号进行分隔，这样再送入`CountVectorizer`类中进行处理。

##### jieba

jieba包的`jieba.cut()`方法讲传入的字符串类型的中文字符串进行分词操作，返回一个对象，可以使用`list()`函数讲这个结果对象强转成list方便后续的操作。

分隔符的`.join()`方法可以将传入的list参数中的元素拼接成字符串。字符串拼接成功后就可以传入`CountVectorizer`进行类似英文字符串的处理。

#### 字典的特征值化

`sklearn.feature_extraction`包中的`DictVectorizer`类可以对字典进行特征值化。向`DictVectorizer`的实例的`.fit_transform()`方法传入待特征值化的字典，方法返回一个稀疏矩阵。如果想得到list而非特殊矩阵，可以在`DictVectorizer`的构造函数中传入`sparse=False`参数。对`DictVectorizer`的实例调用`.get_feature_names()`可以得到类别的名称。

#### one hot编码

为了消除非数值数据的简单编号可能造成的权重偏差，需要对非数值数据进行one hot编码而非简单的数据编码。

pandas提供了`pd.get_dummies()`方法实现one hot编码。

### 特征预处理

特征预处理中很重要的内容是**无量纲化**处理。

在机器学习算法实践中，往往需要将不同规格的数据转换到同一规格或不同分布的数据转换到某个特定分布需求。无量纲化处理可以提高模型求解速度，提示模型精度。不过决策树和树的集成算法则不需要无量纲化处理。

无量纲化处理的2个重要方式分别是：
- 归一化
- 标准化

#### 归一化

如果每个特征都有同样的权重，那么必须对其进行归一化处理。

归一化的公式为$$X' = \frac{x-min}{max-min}$$ $$X'' = X'\times(mx-mi)+mi$$

通常`mx = 1, mi = 0`。

归一化通过调用`sklearn.preprocessing`里的`MinMaxScaler`类实现归一化。`MinMaxScaler`的实例调用`.fit_transform()`方法对传入的特征矩阵参数进行归一化处理。

归一化的突出缺点在于：如果数据中存在的异常值较多，对最大值和最小值的影响很大，严重影响归一化的结果。

#### 标准化

概率论中有一种非常重要的分布——正态分布。如果能够在数据预处理阶段将数据处理成标准正态分布就再好不过了。

通过分析标准化的过程，即标准正态分布处理过程，不难发现异常值对标准化的影响不大。

标准化通过调用`sklearn.preprocessing`里的`StandardScaler`类进行标准化。`StandardScaler`类的实例调用`.fit_transform()`方法对传入的特征矩阵参数进行归一化处理。

#### 归一化 or 标准化？

大多数机器学习算法需要标准化进行数据预处理，而归一化在不涉及距离度量、梯度、协方差计算以及数据需要被压缩到特定区间时被广泛使用，如数据图像中量化像素强度。

### 特征选择

特征选择的第一步应该是与数据提供者联系，与相关业务人员交流，通过业务常识直接筛选特征。

特征选择能够精简特征的个数，取出相关度较高的冗余特征

通过常识进行第一步选择后，还可以通过数学方法进一步选择。

常用的方法有：
- filter过滤式
- embedded嵌入式
- PCA降维

#### filter过滤式

过滤式通过特征数值的方差进行筛选。如果某个特征本身方差很小，就表示这个特征对于区分数据样本的作用非常有限，故应该首先过滤这种无用的特征。

`sklearn.feature_selection`包中的`VarianceThreshold`类可以基于给定方差选择特征列，`VarianceThreshold`类的构造函数接收一个方差阈值，经过这个`VarianceThreshold`实例的`.fit_transform()`方法处理后的特征数据中只存在方差值大于或等于阈值的特征列。

#### PCA降维

降维的维度值就是特征的种类。PCA可以削减回归分析或聚类分析中特征的数量。

`sklearn.decomposition`包中的`PCA`类实例通过调用`.fit_transform()`方法对传入的数据进行PCA降维处理。

# Day 4

## 训练集、测试集划分

为了测试模型经过训练后在陌生数据上的表现能力，需要对原本的数据集进行划分，分成**训练集**和**测试集**。一般来说，数据集中80%作为训练集，20%作为测试集。

`sklearn.model_selection`包提供`train_test_split()`函数进行训练集和测试集划分，如下所示

```python
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=2020)
```

其中
- `x_train`为函数返回的**训练集特征**数据
- `x_test`为函数返回的**测试集特征**数据
- `y_train`为函数返回的**训练集标签**数据
- `y_test`为函数返回的**测试集标签**数据
- `features`为待划分的**特征**数据
- `target`为待划分的**标签**数据
- `test_size=`传入测试集所占比例，例如取值`test_size=0.2`表示原数据集中20%划作测试集
- `random_state=`传入随机数种子，函数按照随机数种子生成的随机数随机划分原数据集

## KNN分类算法

KNN方法是一个**有监督的分类算法**，采用不同特征值之间的距离方法进行分类。

KNN模型接受一个训练样本集，样本集中每个数据都存在标签。

输入新的无标签数据后，将新数据的每个特征与样本集中数据的每个特征进行比较，然后提取出样本集中特征最相似的数据的标签。

一般只选择K个最相似的数据，通常K不大于20。

最后这K个数据的分类标签中出现最多的标签被赋予新的数据，作为新数据的标签。

`sklearn.neighbors`包中提供了`KNeighborsClassifier`类实现了KNN分类算法。类的实例接受参数K，使用`.fit()`方法训练模型。

使用KNN算法时，可以尝试对数据进行标准化以改善算法表现。

## 超参数

算法模型中有一些参数是事先选取的，不依赖于特定的数据集，这些参数称为超参数。

超参数的变化会直接影响模型的表现。

为了寻找最佳的超参数取值，最简单的方法是对超参数进行枚举，并记录算法相应的表现，以此绘制学习曲线。

通过对KNN模型不同K值的结果分析不难得出：
- 若K值较小，则模型容易发生过拟合，也就是预测结果对于邻近的数据点非常敏感
- 若K值较打，则模型的泛化能力相对较强，距离较远的数据点对预测结果也产生一定影响

因此在应用中，K值一般取较小的值，并且采用下面的交叉验证法来选取最优的K值。

## K折交叉验证

K折交叉验证的目的是为了寻找最合适的超参数取值。

K折交叉验证将样本的数据集交叉的划分出不同的训练集和验证集，使用拆分后的不同的训练集和验证集分别测试模型的表现，并将表现取均值作为此次交叉验证的结果。对于不同的超参数都可以进行这样的交叉验证，由此选取合适的超参数。

K折交叉验证的大致过程如下：
- 将原始数据划分出的**训练集**平分成K个等分
- 使用其中的1份作为**验证集**，其余作为**训练集**
- 用此时的**训练集**训练模型，用**验证集**测试模型表现，记录此次表现结果
- 选取尚未选过的某一等份作为验证集，其余作为训练集，重复上一步，直到所有等分均被单独做过验证集
- 收集到的模型表现结果取均值，作为本次交叉验证的最后结果

`sklearn.model_selection`包提供`cross_val_score()`进行交叉验证，返回表现结果的均值。

`cross_val_score()`接受3个参数：
- `estimator=`模型对象
- `X=`待交叉验证划分的特征训练集
- `y=`待交叉验证划分的标签训练集
- `cv=`划分的折数K

对KNN模型使用K折交叉验证如下所示

```python
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.neighbors import KNeighborsClassifier

# features是原始特征集，target是原始标签集
x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=2020)
# 进行5折交叉验证
cross_val_score(KNeighborsClassifier(n_neighbors=5), x_train, y_train, cv=5)
```

## 图片数据的等比例压缩和扁平化处理

### 等比例压缩

假设目前图片的数据以`numpy.ndarray`的类型存储，而且它的大小不符合模型要求（如模型要求(28, 28)而图片目前大小为(130, 105)。为了使之适应模型需要，需要调用`scipy.ndimage`包中的`ndimage`类的`zoom()`进行压缩处理）

`ndimage.zoom()`函数需要2个参数：
- 待处理图片array类型的数组
- `zoom=`接受一个tuple，(目标行大小/原始行大小, 目标列大小/原始列大小)

### 扁平化处理

由于`sklearn`的模型在训练和预测时都需要传入二维数组，为此需要对图片进行相应扁平化处理，使得每一行表示一张图片。扁平化处理不需要额外函数，`numpy.ndarray`自带的`.reshape()`方法就可以实现。

```python
# img_arr的大小是(28, 28)，为了符合模型要求，需要将其转化为(1, 28*28)
img_arr.reshape((1, 784))
```

## 线性回归

回归问题与分类问题不同，回归问题的目标值是连续性的。例如：
- 房价预测问题
- 销售额预测问题

值得注意的是，一组特征维度为n的特征集，经过回归处理后，数据集的权重系数会有n个。

标准的线性关系模型为：

$$y = b + \sum_{i=1}^{n} w_i \times x_i$$

其中w称为权重，而b可以变换为$b = w_0 \times x_0, x_0 = 1$，那么模型可以简化为

$$y = \sum_{i=0}^{n} w_i \times x_i$$

结合向量内积的定义，我们可以得出更一般的形式。

假设有m个样本，写成矩阵的形式就是：

$$
\mathbf{X} = 
\begin{bmatrix}
  1 & x_1^1 & x_1^2 & ... & x_1^n \\\
  1 & x_2^1 & x_2^2 & ... & x_2^n \\\
  ... & ... & ... & ... & ... \\\
  1 & x_m^1 & x_m^2 & ... & x_m^n
\end{bmatrix}
$$

$$
\mathbf{y} = 
\begin{bmatrix}
  y_1 \\
  y_2 \\
  ... \\
  y_m
\end{bmatrix}
$$

其中X为特征集，每列代表一个特征（除第1列），每行代表一个数据。Y为标签数据向量。

权重w也可以写成向量的形式：

$$
\mathbf{w} = 
\begin{bmatrix}
  w_0 & w_1 & w_2 & ... & w_n
\end{bmatrix}
$$

由此，线性模型的公式可以写为

$$
\mathbf{y} = \mathbf{X}\mathbf{w}^{\mathrm{T}}
$$

### 减小误差的方法

回归算法是一个迭代算法，迭代是重复反馈过程的活动，目的通常是为了逼近所需目标或结果，每一次迭代得到的结果会作为下一次迭代的初始值。

回归算法就是在不断的自身迭代来减少误差以使得回归算法的预测结果可以越发的逼近真是结果。

为了通过迭代减少误差，首先需要定义误差的度量标准——损失函数：

$$
\sum_{i=1}^{m} (y_i - \textbf{X}_i \textbf{w})^2
$$

由上述公式不难看出，误差的大小与线性回归方程中的系数w有直接关系，由此，问题转化成——如何求解方程中的w使得误差可以最小。

### L0，L1，L2范式

- L0是向量中非0元素的个数
- L1是向量中各个元素绝对值之和
- L2是向量中各元素的平方和然后求平方根

我们的损失函数代表了L2范式的平方结果， 因此我们的求解目标就转化成了

$$
min_w || \mathbf{y} - \mathbf{X}\mathbf{w} ||_2^2
$$

这个表达式一般称作SSE（误差平方和）或者RSS（残差平方和）。

### 最小二乘法

现在问题转化成了求解让RSS最小化的参数向量w，这种方法称为最小二乘法。

通过最小二乘法计算出的w表达式为：

$$
\mathbf{w} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^{\mathrm{T}} \mathbf{y}
$$

`sklearn.linear_model`包中`LinearRegression`类可以提供一个线性回归的模型实例，实例使用`.fit()`方法进行训练。

需要注意的是回归问题中一般不需要进行无量纲化。

## 回归算法的评价

回归算法的评价指标可以从2个角度来看待回归的结果：
- 是否预测到了正确或者接近正确的数值
- 是否拟合到了足够的信息

以上2种角度，分别对应着不同的模型评估指标。

### 是否预测到了正确或者接近正确的数值

首先考虑残差平方和RSS是否是一个合适的指标。RSS反映了预测值与真实值之间的偏差，但是它是一个无界的和，可以无限大或无限小。所以`sklearn`使用RSS的变体均方误差（mean squared error）衡量预测值和真实值的差异。

$$
MSE = \frac{1}{m} \sum_{i=1}^{m} (y_i - \hat{y}_i)^2
$$

不难看出均方误差是每个样本量上的平均误差。有了这个平均误差，就可以将其与标签的取值范围一起比较，以此获得一个较为可靠的评估依据。

在`sklearn`中有2种常见的函数使用了这个评估方式：
- `sklearn.metrics`包中提供了`mean_squared_error`类
- 交叉验证的类`cross_val_score`向参数`scoring`赋值`neg_mean_squared_error`

### 是否拟合到了足够的信息

对于回归类算法而言，仅探索数据预测是否准确是不足够的。除了数据本身的数值大小外，我们还希望模型能够捕捉到数据的规律，而这是无法用MSE衡量的。

回忆在特征工程中的特征选择环节中我们曾经根据方差来衡量数据的信息量。如果方差越大代表数据的信息量越大，这个信息量中不仅包含了数值本身的大小，还包含了我们希望模型捕捉到的那些规律。为此，我们定义了一种新的指标：

$$
R^2 = 1 - \frac{RSS}{\sum_{i=0}^{m} (y_i-\bar{y})^2}
$$

$R^2$的分子表示的是真实值和预测值之间的差值，也就是我们的模型未能捕捉到的信息量，而分母是真实标签所带的信息量，所以其衡量的是

$$
1 - \frac{模型未捕获的信息量}{真实标签中所带的信息量}
$$

这个指标越接近1越好。

`sklearn`中有常见的以下3种方式调用这个指标：
- `sklearn.metrics`包中有`r2_score`类
- `LinearRegression`类中的`score`
- 在交叉验证中输入`r2`

# Day 5

## 欠拟合 & 过拟合

### 欠拟合

一个模型在训练数据上不能获得很好的拟合结果，但在训练数据之外的数据集上也不能很好的拟合数据，则我们认为这个模型出现了欠拟合现象。（如模型过于简单）

欠拟合常用的解决方法是增加样本的特征数量，如采用多项式回归。

### 过拟合

一个模型在训练数据上能够获得非常好的拟合，但是在训练数据以外的数据集上却不能很好的拟合数据，则我们认为这个模型出现了过拟合现象。（如模型过于复杂）

过拟合的解决方法有：
- 进行特征选择，消除关联性大的特征
- 正则化之岭回归

## 多项式回归

线性回归模型有个很明显的缺点——回归出的是线性模型，但是数据的规律更多的有可能是非线性的。线性模型无法捕捉到特征与目标之间的非线性关系，因此表现往往不佳。

为了解决这个问题，经常需要使用高次多项式建立模型拟合曲线。次数过高会导致过拟合，次数不够则会导致欠拟合。

如二次多项式函数：
$$
y = \sum_{i=0}^{n} w_i x^i
$$

实际上，多项式回归可以看作特殊的线性模型，即把高次项看作新的特征，这样形式上又回到了线性回归的模型。

多项式回归的最高次选择取决于样本本身，需要选择一个合适的最高次项，如果选择的次数过高，则会导致过拟合，次数过低又会导致欠拟合。

`sklearn.preprocessing`包中提供了`PolynomialFeatures`类来构造更高次的特征。它是用多项式的方法来进行的，如有a，b两个特征，那么它们的2次多项式为$(1, a, b, a^2, ab, b^2)$

`PolynomialFeatures`中最重要的参数`degree=`用来控制多项式的度。

## 过拟合处理：正则化

将过拟合的凹凸幅度减小可以将过拟合曲线趋近于拟合的曲线，而凹凸程度主要是由高次项导致的，那么正则化就是调小高次项的权重以解决过拟合的问题。

`LinearRegression`类无法进行正则化，所以这个模型可能导致过拟合且其不提供解决方法。

### L2正则化

正则化的回归模型（Ridge岭回归）使用L2正则化处理过拟合问题。

`sklearn.linear_model`包中`Ridge`类提供了岭回归模型，传入的参数主要有`alpha=`，用于指示正则化的力度，力度越大，表示高次项的权重越接近于0。

## 模型的保存和加载

`sklearn.externals`包中提供了`joblib`用于保存和加载模型。
- `joblib.dump(model, 'name.m')`用于保存模型
- `joblib.load('name.m')`用于加载模型

`pickle`包中也提供了模型保存和加载的方法

```python
# 模型保存
with open('./name.pkl', 'wb') as fp:
  pickle.dump(model, fp)

# 模型加载
with open('./name.pkl', 'rb') as fp:
  pickle.load(fp)
```

## 朴素贝叶斯

在许多分类算法的应用中，特征和标签之间的关系并非是决定性的。因此，我们希望算法得出的结论并非确定性的结论，而是结果的可能性概率。比如文章分类模型，我们希望模型能够返回某文章属于各个类别的概率。

朴素贝叶斯正是这样的一种算法，它直接衡量标签和特征之间的概率关系的有监督学习算法，是一种专注分类的算法。朴素贝叶斯基于概率论和数理统计的“贝叶斯推断”。

### 贝叶斯推断

贝叶斯推断是一种统计学方法，用来估计统计量的某种性质，是贝叶斯定理的应用。

要理解贝叶斯推断，首先要理解贝叶斯定理

### 贝叶斯定理

贝叶斯定理中非常重要的2个概念是**先验概率**和**后验概率**。

假设这样一个情景，出门堵车的原因有2个：
- 车辆太多
- 交通事故

先验概率就是堵车的概率（不管原因），可以将其看作是结果发生的概率$P(结果)$。

后验概率则是“由果推因”，即$P(原因_i|结果)$

贝叶斯定理里特指的条件概率则是由因推果，即$P(结果|原因_i)$

问题在于后验概率通常难以计算，但是通过贝叶斯公式，我们可以用先验概率、条件概率等计算后验概率，即

$$
P(原因_i|结果) = \frac{P(原因_i, 结果)}{P(结果)} = \frac{P(结果|原因_i) P(原因_i)}{P(结果)}
$$

朴素贝叶斯只适用于特征之间条件独立的情况下，否则分类效果不好。这里的“朴素”指的就是条件独立。

朴素贝叶斯主要被广泛地用于文档分类中。

### 朴素贝叶斯的分类

`sklearn`提供了3种不同类型的贝叶斯模型算法：
- 高斯模型
- 多项式模型
- 伯努利模型

#### 高斯模型

通过假设$P(x_i|Y)$服从高斯分布来估计训练集数据的每个样本特征分到类别Y的条件概率。

`sklearn.naive_bayes`包中提供了`GaussianNB`类作为高斯模型的实现。

实例化`GaussianNB`类时不需要输入任何参数，非常轻量，但这也意味着贝叶斯没有太多的参数可以调整，因此贝叶斯算法的成长空间并不是太大，如果贝叶斯算法的效果不太理想，我们一般会考虑换模型。

`GaussianNB`的实例提供了2个方法返回模型计算的概率值：
- `.predict_proba()`给出每个测试集样本属于每个类别的概率，最大的就是分类结果。
- `.predict_log_proba()`对`.predict_proba()`的对数转化，最大的就是分类结果。

#### 多项式模型

与高斯分布不同，多项式模型主要适用于离散特征的概率计算，需要处理连续性变量时最好采用高斯模型。

值得注意的是，`sklearn`的多项式模型不接受负值输入，所以样本数据如出现负值，要进行归一化处理。

以文章类别分类为例，给定一篇文章，模型需要计算它属于各种类别的概率，也就是$P(类别|文章)$，而文章可以看作词语组成的序列，那么

$$
P(类别|文章) = P(类别|词_1, 词_2, ..., 词_n) \\
= \frac{P(词_1, 词_2, ..., 词_n|类别) P(类别)}{P(词_1, 词_2, ..., 词_n)} \\
= \frac{P(词_1|类别)P(词_2|类别) ... P(词_n|类别)P(类别)}{P(词_1, 词_2, ..., 词_n)}
$$

上述公式有个问题，若训练数据中某类别下某词的出现概率为0，则经过该模型计算后含有该词的某文章属于该类别的概率就是0，这肯定是不合适的，因为仅仅一个词就使得某文章属于某类别的概率直接为0，所以在这种情况下需要进行**拉普拉斯平滑**处理。

$$
P(F1|C) = \frac{Ni+\alpha}{N+\alpha m}
$$

$\alpha$为指定的系数，一般为1，m为训练文档中统计出的特征词个数。

`sklearn.naive_bayes`提供了`MultinomialNB`类实现多项式模型，该类的实例中`alpha=`参数用于指定拉普拉斯平滑系数。

#### TF-IDF

在文本处理和信息检索中，TF-IDF是一种统计方法，用以评估一个单词在一个文档集合或语料库中的重要程度。

TF-IDF实际上是TF*IDF，主要思想是：如果某个词或短语在一篇文章中出现的频率高（即TF高），且在其他文章中很少出现（即IDF高），那么可以认为这个词或短语具有很好的类别区分能力，适合用作分类。

其中，

$$
TF(某词) = \frac{该词在文档中出现的次数}{文档的总词数}
$$

`sklearn.feature_extraction.text`包提供`TfidfVectorizer`类求TF-IDF值。使用该类实例的`.fit_transform()`方法对原特征集进行处理，得出新的特征集。

#### 伯努利模型

伯努利模型与多项式模型非常相似，都用于处理文本分类数据，但由于伯努利处理的是二项分布，所以它更在意“是或否”。伯努利模型每个特征的取值只能是0或1，所以它比`MultinomailNB`多定义一个二值化方法，该方法接受一个阈值并将输入的特征二值化（0或1）

`sklearn.naive_bayes`包中的`BernoulliNB`类提供了伯努利模型。该类的实例同样接受一个拉普拉斯平滑系数，同时它还接受一个`binarize=`参数，可以是数值或不输入，不输入表示特征以全部二值化；输入数值则表示小于等于该值的归为一类，大于的归为另一类。

# Day 6

## IV

IV的全称是information value，中文意思是信息价值或者信息量。

在构建**分类**模型时，经常需要对特征进行筛选，那么如何挑选特征用于模型训练呢？

不难看出，挑选特征最主要和最直接的衡量标准是特征的预测能力。而IV就是一种可以用来衡量自变量预测能力的指标。

假设一个分类问题中，目标变量有2各类别：Y1和Y2。对于一个待分类的样本A，要判断其类别需要一定的信息，假设这个信息总量是I，而这些信息就蕴含在这个样本的特征C1, C2, ... Cn中。对于其中的某个特征Ci，它蕴含的信息越多，它对于判断A类别的贡献就越大，Ci的信息价值就越大，相应地Ci的IV就越大。

为了计算IV，首先需要认识和理解另一个概念——WOE。

### WOE

WOE的全称是weight of evidence，即证据权重。

WOE是对原始特征的一种编码形式。要对一个特征进行WOE编码，需要首先把这个变量进行分组处理（也称离散化、分箱）。分组后，对于第i组，WOE的计算公式如下

$$
WOE_i = ln(\frac{py_i}{pn_i}) = ln(\frac{\frac{y_i}{y_T}}{\frac{n_i}{n_T}})
$$

其中
- $py_i$是这个组中正例样本个数占整个样本集中正例样本个数的比例
- $pn_i$是这个组中负例样本个数占整个样本集中负例样本个数的比例
- $y_i$是这个组中正例样本的数量
- $n_i$是这个组中负例样本的数量
- $y_T$整个样本集中所有正例样本的数量
- $n_T$整个样本集中所有负例样本的数量

计算出一个分组的WOE值后，该分组的IV值计算公式如下所示：

$$
IV_i = (py_i - pn_i) \times WOE
$$

计算出一个特征各分组的IV值后，就可以计算整个特征的IV值

$$
IV = \sum_{i}^{n} IV_i
$$

当我们计算WOE值时，可能出现负值，这说明了当前分组负例样本占比更多；如果WOE值为0，说明当前分组的正负比例和总样本集的正负比例相等。

进一步理解WOE的正负问题，我们可能得出，WOE描述了某一特征的当前分组对于判断该特征是否会响应（也就是所属的类别）起到的作用大小，WOE的正负表示了影响的正负，WOE的绝对值表示了这种影响的大小。

所以在计算某特征的总WOE值时，需要将负值的符号消除，也就是对所有WOE的绝对值求和。

进一步分析IV的计算公式，IV在WOE的基础上考虑了各类别的占比所造成的影响。对于特征的某个分组，这个分组的响应与样本整体响应的比例和这个分组未响应的比例（$py_i - pn_i$）相差越大，IV值越大。

之所以考虑各类别占比的影响，是因为有这样一种情况，在特征的某一取值时计算出的WOE非常大，但是这个取值的个数太少了，在全体样本集中的比例太少了，那么对于样本整体而言，这个特征的预测能力可能也没有那么强

## 分箱

数据分箱是一种数据预处理技术，用于减少次要观察误差的影响，是多个连续值分组为较少数量分组的办法，也就是将连续性特征离散化。

### 分箱的作用和意义

特征离散化后，模型会更稳定，相比连续性时，单个数据一般只会影响单个区间，而且特征离散化之后，起到了简化模型的作用，适当降低了模型过拟合的风险。

`pandas`包提供了`pandas.cut`方法进行分箱。`pd.cut()`根据值本身来选择箱子的均匀间隔。

`pandas`包还提供了`pandas.qcut()`方法进行分箱，该函数根据值的频率来选择箱子的间隔，力求做到每个区间中数据的数量相同。

分箱后就可以求解WOE和IV值。如果需要对连续性特征进行离散化，需要对分箱后的每组数据进行IV编码，然后再进行训练。

## 类别分布不均衡处理

如果同一组样本中不同类别的样本量差异非常大，则属于类别分布不均衡。类别分布不均衡将导致样本量少的分类包含的特征过少，并很难从中提取规律，模型的准确性会很差。

解决方法有：
- 过抽样
- 欠抽样

### 过抽样

过抽样通过增加分类中少数类样本的数量来实现样本均衡，比较好的方法有`SMOTE`算法。

`SMOTE`算法的思想是合成新的少数类样本，合成的策略是对于每个少数类样本，从它与最近邻的同类样本的连线上随机选取一个新样本作为新合成的少数类样本。

`imblearn.over_sampling`包中提供`SMOTE`类实现过抽样，`SMOTE`类接受一个参数`k_neighbors`，表示找出类别少的样本点周围最近的K各邻居。

`SMOTE`的实例使用`.fit_sample()`方法进行过抽样，该方法接受一个特征DataFrame和一个标签Series。

### 欠抽样

欠抽样与过抽样相反，它通过减少分类中多数类样本的数量来实现样本均衡。

`imblearn.under_sampling`包提供`RandomUnderSampler`类实现欠抽样，使用方法与`SMOTE`类类似。

## 逻辑回归

回忆之前使用过的线性回归，是一个用于回归问题的典型模型。那么回归模型是否可以用于分类呢？答案是肯定的，逻辑回归就是这样一种模型，通过引入Sigmoid函数实现回归模型用于分类问题。

### Sigmoid函数

Sigmoid函数将线性回归方程变换为g(z)，并将g(z)的值分布在(0, 1)之间，当g(z)接近0时样本的标签为0，当g(z)接近1时样本的标签为1，这样我们就用回归的形式实现了一个分类模型。

$$
g(z) = \frac{1}{1+e^{-z}}
$$

Sigmoid函数是一个S型的函数，当自变量z趋近于正无穷时，因变量趋近于1；反之趋近于0。它能够将任何实数映射到(0, 1)区间，使其可用于将任意值函数转换成更适合二分类的函数，因此这个函数也可以用于归一化。

### 逻辑回归与线性回归的关联

将线性回归中的$z = \mathbf{\theta}^{\mathrm{T}} \mathbf{x}$代入，得到了二元逻辑回归的一般形式：

$$
g(z) = y(x) = \frac{1}{1+e^{-\mathbf{\theta}^{\mathrm{T}} \mathbf{x}}}
$$

我们构造这样一个形似几率

$$
\begin{aligned}
ln \frac{y(x)}{1-y(x)} &= ln \frac{1}{e^{-\mathbf{\theta}^{\mathrm{T}} \mathbf{x}}} \\
&= \mathbf{\theta}^{\mathrm{T}} \mathbf{x} 
\end{aligned}
$$

不难发现，这个式子就是线性回归的形式，因此逻辑回归是由线性回归变化而来。

### 逻辑回归的特点

- 逻辑回归对线性关系的拟合效果非常好，如信用卡欺诈问题，评分卡制作问题，营销预测问题等，然而逻辑回归对非线性数据的拟合效果非常差
- 逻辑回归的计算快，效率优于SVM和随机森林
- 逻辑回归可以返回不固定的结果而是返回(0, 1)之间的一个数，这个数可以视作概率

### 逻辑回归的损失函数

在逻辑回归分类时，不管原始样本中的类别如何表示，逻辑回归统一将其视为0/1类别。

因为逻辑回归也采用了寻找特征和目标之间的关系，每个特征也有权重w，那么也会存在真实值和预测值之间的误差，然而逻辑回归的损失函数和线性回归的损失函数并不一致。逻辑回归采用的是**对数似然损失函数**。

$$
cost(h_{\theta} (x), y) = 
\begin{cases}
-log(h_{\theta}(x)) & if & y = 1 \\
-log(1-h_{\theta}(x)) & if & y = 0   
\end{cases}
$$

注意一下，没有求解参数需求的模型是没有损失函数的，如CNN。

综合一下，逻辑回归的完整损失函数为

$$
cost(h_{\theta} (x), y) = \sum_{i=1}^{m} (-y_i log(h_{\theta} (x)) - (1 - y)log(1 - h_{\theta} (x)))
$$

我们使用求解参数$\theta$的迭代方法是梯度下降。

### 正则化

正则化是为了解决过拟合问题。常用的有L1正则化和L2正则化，它们分别通过在损失函数后加上参数向量$\theta$的L1范式和L2范式的倍数来实现，这个增加的范式被称为“正则项”，实质是一种惩罚项。

加入L1范式的损失函数如下所示。

$$
cost(h_{\theta} (x), y)_{L1} = cost(h_{\theta} (x), y) + \frac{1}{C} \sum_{j=1}^{n} |\theta_j| & (j \geq 1)
$$

加入L2范式的损失函数如下所示。

$$
cost(h_{\theta} (x), y)_{L2} = cost(h_{\theta} (x), y) + \frac{1}{C} \sqrt(\sum_{j=1}^{n} (\theta_j)^2) & (j \geq 1)
$$

公式中C是用于控制正则化程度的超参数。这里j不需要取0，因为参数向量中第一个参数不需参加正则化。

L1正则化有可能将参数惩罚到0，L2正则化只会让参数尽量小，不会使其为0。

通常来说，为了防止过拟合，L2的正则化已经足够，如果选择L2正则化后还是过拟合，可以选用力度更大的L1正则化。

### API

`sklearn.linear_model`包提供了`LogisticRegression`类实现逻辑回归，这个类的实例可以接受如下参数：

- `penalty=`指定使用的正则化方法，不填写默认`'l2'`。若选择`'l1'`，参数`solver`仅能使用求解方式`'liblinear'`和`'saga'`；若选择`'l2'`则无此限制。
- `C=`惩罚项参数，必须是大于0的浮点数，默认为1.0。从正则化公式不难看出，C越小，正则化力度越大。
- `max_iter=`梯度下降中能走的最大步数，默认为100步。这个参数最好的选取方法是通过绘制学习曲线。
- `solver=`用于指定求解器，即求解最小值的算法
- `multi_class=`用于告知模型要处理的分类问题的类型。一共有3种取值。
  - `ovr`表示分类问题是二分类，
  - `multinomial`表示分类问题是多分类问题
  - `auto`表示模型根据其他参数的取值自动确定。

# Day 7

## 分类模型的评价指标

分类模型的评价指标除了之前常见的一带而过的准确率之外，还有如下几种方式：
- 准确率
- 精准率
- 召回率
- f1-score
- AUC曲线

### 混淆矩阵

在分类任务下，预测结果和真实结果之间存在4种不同的组合：
- 真正例（TP）：真实值是**正**，预测值为**正**的比例（正例识别为正例，√）
- 伪正例（FP）：真实值是**负**，预测值为**正**的比例（负例识别为正例，×）
- 伪反例（FN）：真实值是**正**，预测值为**负**的比例（正例识别为负例，×）
- 真反例（TN）：真实值是**负**，预测值为**负**的比例（负例识别为负例，√）

$$
真正例率TPR = \frac{TP}{TP + FN} \\
伪反例率FPR = \frac{FP}{FP + TN}
$$

不难得出，TPR越大越好，FPR越小越好。

### 准确率

$$
Accuracy = \frac{TP + TN}{TP + FN + FP +TN}
$$

准确率描述的是预测正确的比例，很多模型自带的方法`.score()`使用的就是准确率。

准确率存在这样一个问题，以时事的新冠病毒检测举例，因为目前在中国，阳性的标签占比非常之小，所以假设采取这样的检测方式：将所有标本的检测结果判定为阴性，那么这种方法的准确率将会非常之高，但毫无疑问这种方式是不可取的。

### 召回率

召回率能够很好的解决上述准确率在标签分布非常不均衡时遇到的指标无效性问题。

$$
召回率 = \frac{TP}{TP + FN}
$$

召回率描述了真正为正例的样本中预测结果为正例的比例，即正样本中有多少被找出。

`sklearn.metrics`包中提供了`recall_score`类实现召回率的计算。

### 精确率

注意区分准确率（Accuracy）和精确率（Precision）。

$$
Precision = \frac{TP}{TP + FP}
$$

精确率描述的是预测为正例的样本中预测正确的比例

`sklearn.metrics`包中提供了`accuracy_rate`类实现精确率的计算。

### f1-score

有时精确率和召回率的指标会产生冲突，这时我们可以采用f1-score来描述二分类模型的分类效果。f1-score是精确率和召回率的调和平均数。

f1-score的公式如下所示

$$
f1-score = \frac{1}{\frac{1}{recall} + \frac{1}{pre}} = \frac{2 \times pre \times recall}{pre + recall}
$$

f1-score的最大值为1，最小值为0，它反映了模型稳健性，是一个综合的评价标准。

`sklearn.metrics`提供了`f1_score`类实现f1-score。

### AUC

AUC也是一个模型评价标准，**只能用于二分类模型**的评价。AUC应用较广，原因是很多机器学习的分类模型计算结果都是概率的形式，那么对于分类问题，就需要设定一个**阈值**来判定分类，这个阈值的判定就会对我们模型的表现产生影响。其中逻辑回归的阈值默认为0.5

AUC（Area under Curve）指的是ROC曲线（接收者操作特征曲线）下边的面积。

ROC曲线就是以FPR为横坐标，TPR为纵坐标绘制的一条曲线，理想情况下最佳的分类器应该尽可能地处于左上角，这就意味着分类器在FPR很低的同时获得了很高的TPR。

AUC则是ROC曲线下的面积，取值固定在(0, 1)。AUC的值越大越好。

`sklearn.metrics`包中提供了`roc_auc_score`类实现了AUC。

## KMeans聚类算法

在此之前，我们所学习到的算法模型都是有监督学习算法，即模型需要的样本数据既有特征矩阵X，也有标签向量y。同时机器学习中存在着一类无监督学习算法。无监督学习算法的模型只要特征矩阵X即可，不需要真实的标签y。聚类算法是无监督学习中的代表之一。

聚类算法的目的是将数据划分成有意义或有用的组（簇）。这种划分可以基于需求完成，也可以单纯帮助我们探索数据的自然结构和分布。

下表总结了聚类和分类的区别

|          | 聚类                                                                                                 | 分类                                                       |
|----------|------------------------------------------------------------------------------------------------------|------------------------------------------------------------|
| 核心     | 将数据分成多个组，探索每个组的数据是否有联系                                                         | 从已分组的数据中学习，把新数据放到已经分好的组中           |
| 学习类型 | 无监督                                                                                               | 有监督                                                     |
| 典型算法 | KMeans                                                                                               | 贝叶斯，逻辑回归                                           |
| 算法输出 | 聚类结果总是不确定的，不一定总是能反映数据的真实分类，同样的聚类，根据不同的业务需求，结果好坏不确定 | 分类结果是确定的，优劣是客观的，不是根据业务或算法需求决定 |

### 簇和质心

KMeans算法将一组N个样本的特征矩阵X划分为K个无交集的簇，直观上看簇是一个又一个聚集在一起的数据，在一个簇中的数据就认为是同一类。簇就是聚类的结果。

簇中所有数据的均值就是这个簇的质心。在n维平面中，所有数据在某一维的坐标的均值就是质点在这个维度的坐标值。

在KMeans算法中，簇的个数K是一个超参数，KMeans的核心任务就是找出最优的K个质心，并将其余数据合适地分配到这K个质心所代表的簇中。具体流程如下：
- 随机抽取K个样本作为最初的质心
- 开始循环
  - 将每个样本点分配到离它们最近的质心，生成K个簇
  - 对于每个簇，计算该簇的新质心
  - 当质心的位置不再发生变化，则迭代停止，聚类完成

当聚类完成时，以后的每次迭代各个簇都不会再发生变化，质心也不再发生变化。

我们认为聚类算法分出的每个簇中的数据具有相似性，而不同的簇之间的数据之间的差异较大。所以聚类算法应该追求“簇内差异小，簇外差异大”，而这个差异由样本点到其所在簇的质心的距离来衡量。

对于一个簇来说，所有样本点到质心的距离和越小，就可以认为这个簇中的样本越相似，簇内差异越小。

### 损失函数？

如果我们采用欧式距离，则一个簇中所有样本点到质心的距离的平方和为簇内平方和，使用簇内平方和就可以表示簇内差异大小。

如果将一个数据集中所有簇的簇内平方和相加，就得到了整体平方和，又叫做total inertia，这个值越小，代表着每个簇内样本越相似，聚类的效果越好。

KMeans追求的是求解能让簇内平方和最小化的质心。实际上，在质心不断变化不断迭代的过程中，整体平方和在减小，如此当整体平方和最小的时候，质心就不再发生变化了。

因此我们可以认为求解质心的过程也就是最小化整体平方和的过程。但是损失函数的本质是衡量模型的拟合效果，只有有着求解参数需求的算法，才会有损失函数。而KMeans算法不存在求解说明参数，本质也并非拟合数据，而是对数据进行一种探索。

### API

`sklearn.cluster`包提供了`KMeans`类实现KMeans聚类算法。

该类的实例有2个重要参数：
- `n_clusters=`是KMeans中的K，表示着我们告诉模型我们要分几类。这是KMeans当中唯一一个必填的参数，默认是8类。通常在开始聚类之前，我们也不知道`n_clusters`究竟是多少。
- `random_state=`用于初始化质心的生成器。

当我们刚拿到数据集，如果可能的话，我们希望能够通过可视化先观察一下这个数据集的分布，以此来为聚类时输入的n_clusters做一个参考。

### KMeans对结果的预测

KMeans算法通常是不需要预测结果，但有时我们需要使用`.predict()`进行预测操作，因为有时数据量过大，我们可以通过少量数据先确定质心，然后剩下的数据之间用`.predict()`确定质心。

## 聚类算法的效果

聚类模型的结果不是某种标签输出，且其结果不确定，优劣与否很大程度上取决于业务需求或算法需求。那这样是否就没有衡量指标呢？

之前提到聚类算法的目的是保证“簇内差异小，簇外差异大”，这提示着我们可以通过衡量与簇相关的差异来衡量聚类的效果。

### 轮廓系数

轮廓系数能够衡量：
- 样本与其自身所在的簇中其他样本的相似度a，等于样本与同一簇中所有其他点之间的平均距离
- 样本与其他簇中的样本的相似度b，等于样本与下一个簇中所有点之间的平均距离

根据“簇内差异小，簇外差异大”的目标，我们希望b永远大于a，并且这种差距越大越好。

轮廓系数是最常用的聚类算法的评价指标，它是对每个样本来定义的。轮廓系数的表达式为：

$$
s = \frac{b - a}{max(a, b)}
$$

很容易理解轮廓系数范围是(-1, 1)，值越接近1表示样本与自己所在的簇中的样本很相似，并且与其他簇中的样本不相似；值越接近-1就表示样本与簇外的样本更相似。

如果一个簇中大多数样本具有比较高的轮廓系数，则簇和整个数据集的平均轮廓系数越高，则聚类是合适的

`sklearn.metrics`包提供`silhouette_score`类计算轮廓系数，它的返回值是一个数据集中所有样本的轮廓系数的均值。

这个包中同样提供`silhouette_sample`类，它的参数与轮廓系数一致，但返回的是数据集中的每个样本自己的轮廓系数。

## SVM

SVM是机器学习中获得关注最多的算法，也是最接近深度学习的机器学习算法。在图像和文本识别问题的表现很好。

SVM的原理这里简略不计。

不是所有的数据都线性可分，因此需要对其进行升维变化，让数据从原始空间投射到新空间，升维后可以明显找出一个平面将数据切分开。

核函数能够帮助我们寻找合适的新空间投射函数。

常用的核函数有以下4种：

| 输入      | 含义       | 解决问题 |
|-----------|------------|----------|
| 'linear'  | 线性核     | 线性     |
| 'poly'    | 多项式核   | 偏线性   |
| 'sigmoid' | 双曲正切核 | 非线性   |
| 'rbf'     | 高斯径向基 | 偏非线性 |

- 线性核函数和多项式核函数在线性数据集表现良好，在非线性集表现一般
- sigmoid核的表现不好，较少使用
- rbf核在任何数据上表现都不错，属于首先考虑使用的核函数

`sklearn.svm`包中提供了`SVC`类实现SVM，该类实例中重要的参数有：
- `kernel`使用的核函数
- `C`正则化力度
- `class_weight`对类别分布不均衡的处理

初遇数据集时，很难区分数据集是否线性可分，为此我们可以先使用PCA降维将其降到适合可视化的维度，然后在图像中查看其是否线性可分。

# Day 8

## 决策树

决策树是一种有监督学习方法，它能够从一系列由特征和标签的数据中总结出决策规则，并用树状图的结构来呈现这些规则以解决分类和回归问题。

决策树算法的本质是一种树结构，只需问一系列问题就可以对数据进行分类。

在决策过程中，我们一直在对记录的特征进行提问，最初问题所在的节点叫做根节点，在得到结论前的每个问题都是中间节点，得到的每个结论叫做叶子节点。

一些节点类型的定义：
- 根节点：没有入边，有出边，包含最初的针对特征的提问
- 中间节点：既有入边也有出边，出边有很多条
- 叶子节点：有入边没有出边，每个叶子节点都是一个类别标签
- 子节点和父节点：在2个相连的节点中，更接近根节点的是父节点，另一个是子节点

决策树算的核心是要解决2个问题：
- 如何从数据表中找出最佳节点或最佳分支？
- 如何让决策树停止生长，防止过拟合？

原则上来说，任一数据集上的所有特征都可以用来做分支，特征上的任意节点又可以自由组合，所以一个数据集上可以发展出非常多棵决策树，其数量可达指数级。在这些树中，总有一棵树比其他树分类效果好，这样的树叫做“全局最优树”。

通过枚举法求解全局最优树是不现实的，为此机器学习使用了一些贪心策略的算法用于求解决策树问题。

### ID3算法

常见的决策树算法有ID3，C4.5和CART，其中理论基础最完善的是ID3和C4.5。

为了衡量决策树的好坏，常采用“不纯度”作为指标。不纯度基于叶子节点来计算，所以树中每个节点都有不纯度，且子节点的不纯度一定低于父节点的不纯度，即同一棵决策树中，叶子节点的不纯度最低。

决策树的每个根节点和中间节点都会包含一组数据，在这组数据中，如果有某一类标签占有较大比例，我们就说该节点越纯，分支分得好；如果各类标签都相对平均，则该节点“不纯”。

计算不纯度的方法是通过计算信息熵实现的。

信息量的值在信息论中被称为信息熵，计算公式如下

$$
H = - \sum_{i=1}^{n} p(x_i) log_2 p(x_i)
$$

信息熵则是一种信息的度量方式，表示信息的混乱程度，也就是说，信息越有序，信息熵越低（信息量和不纯度越低）

### 信息增益

为了更有效的得出结论，不难得出，对分类效果最大的特征要放在离根最近的位置。**信息增益**恰恰就是用来衡量决策树中节点的重要性。

如果一个节点减少分类的不确定性能力越强，该节点就越重要，所以信息增益也就是不纯度的减少量。

因此在每次分支时都要计算每个特征的信息增益，并采用信息增益最大的特征。

### 决策树的构建过程

在构建决策树的过程中，为了找到当前信息增益最大的特征，必须评估每个特征。在构造过程中，我们需要解决3个问题：
- 选择哪个属性作为根节点
- 选择哪些属性作为子节点
- 何时停止并得到目标状态，即叶节点

### ID3的局限性

- ID3不难处理连续性数据（连续性标签），若要处理连续性数据，首先需要对连续变量进行离散化
- 对缺失值敏感
- 没有剪枝的设置，容易导致过拟合。

### CART（基尼函数）

CART的全称是分类和回归树，既可用于分类，也可用于回归。

在实际使用中，信息熵和基尼系数的效果基本相同，信息熵的计算比基尼系数缓慢，而且信息熵计算出树更加“精细”，因此信息熵的决策树对于高维数据或噪声较多的数据容易过拟合。

### API

`sklearn.tree`包提供`DecisionTreeClassifier`实现决策树。该类的实例有下述几个重要参数：
- `criterion=`选择`entropy`或`gini`
- `random_state=`固定决策树。因为决策树在默认参数的构建分支过程中并不是每次都使用全部特征，而是随机选取一部分特征进行分支，这样每次生成的树也不同。
- `splitter=`用来控制决策树生成时的随机程度
  - `best`表明树更倾向于选择更重要的特征
  - `random`表明树的随机性更强
- `max_depth=`用于限制树的最大深度，超过设定深度的树枝全部被剪掉。这个是使用最广泛的剪枝参数，在高维低样本量时非常有用。使用时建议从3开始尝试
- `min_samples_leaf=`限定一个节点在分支后的每个节点必须包含的训练样本的最小数量，小于这个量的分支不会发生，或者分支会倾向于朝着每个子节点都包含`min_samples_leaf`个样本的方向去发生。一般搭配`max_depth`使用。太小会导致过拟合，太大会阻止模型学习。一般建议从5开始使用。
- `min_samples_split=`限定了每个节点至少包含的样本数量。

`graphviz`包提供了绘制决策树的方法。

## 集成学习

集成学习是时下非常流行的机器学习算法。

集成算法的目标是考虑多个评估器的建模结果，汇总之后得到一个综合结果。

实现集成学习的方法可以分为2类：
- 并行集成方法（bagging）
  - 参与训练的基础学习器并行生成，通过平均或者投票的方式提高表现。
- 序列集成方法（boosting）

### Bagging

Bagging是一种有放回的抽样方法，每轮从原始样本中有放回的抽取n个训练样本。共进行K轮抽取，得到K个训练集。每次使用一个训练集得到一个模型，K个训练集得到K个模型。

在分类问题中，将上步的K个模型采用投票的方式得到分类结果；对于回归问题，计算上述模型的均值作为最后的结果。

## 随机森林

随机森林是一种以决策树为基础的有监督集成学习方法。它计算开销不大，在分类和回归上的表现也不错。

随机森林的随机性体现在：
- 数据集的随机选择
- 待选特征的随机选取

随机森林的重要作用：
- 可用于分类和回归
- 可以解决过拟合问题
- 可以检测出重用的特征

由于随机性的大量引入，随机森林不容易陷入过拟合，抗噪声能力强，对比其他算法也有一定优势。它的缺点在于当决策树数量过多时，训练所需的空间和时间比较大。

`sklearn.ensemble`包提供了`RandomForestClassifier`类用于分类，`RandomForestRegression`类用于回归。

以`RandomForestClassifier`类的实例接受的重要参数有：
- `n_estimators=`为森林中树木的数量，值越大效果往往越好。默认值为100
- `random_state=`用于固定随机森林
- `oob_score=`这个值设为`True`时表示用袋外数据进行测试（袋外数据就是在bagging抽样时从未被抽取的样本数据）

