---
title: '强化学习#1 模型基础'
date: 2019-11-29 16:48:36
categories: [AI, RL]
tags:
- AI
- RL
mathjax: true
---
# 引言

这个关于RL的系列，既是平时学习的总结回顾，也作为复习资料。因为平时学业的原因，RL的学习常常会中断不少时间，遗忘率较高，写下这个系列，便于复习与回顾。

参考博客[刘建平Pinard](https://www.cnblogs.com/pinard/)

[参考文章](https://www.cnblogs.com/pinard/p/9385570.html)

<!-- more -->

# 强化学习与其他机器学习方法的异同

Reinforcement Learning是与Supervised Learning, Unsupervised Learning并列的Machine Learning方法

## RL vs Supervised Learning

1. RL没有预先准备的输出值以供学习。RL只有reward，reward不是事先给出，而是在agent与environment交互后得到的。
2. RL的每一步(step)与时间顺序密切相关；Supervised Learning的数据之间往往相互独立。

## RL vs Unsupervised Learning

1. Unsupervised Learning无输出值也无reward，只有数据特征。
2. Unsupervised Learning数据之间独立，没有前后依赖关系。

# RL建模

## MDP

> MDP可由元组$(S, A, P, R, \gamma)$描述
> $S$为有限的**状态集**
> $A$为有限的**动作集**
> $P$为状态转移概率
> $R$为回报函数
> $\gamma$为折扣因子，用来计算累积回报

还有一些重要的模型要素如下

1. Policy $\pi$。最常见的策略表达方式是一个**条件概率分布**$\pi(a|s)$，即$\pi(a|s)=P(A_t=a|S_t=s)$
2. $v_{\pi}(s)$，agent在policy $\pi$和state $s$时采取action后的价值(value)。价值函数$v_{\pi}(s)$一般表示为下式，变体在其基础上调整。
   $$v_{\pi}(s)=\mathbb{E}_{\pi} (R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...|S_t=s)$$
3. 探索率$\epsilon$


参考博客[刘建平Pinard](https://www.cnblogs.com/pinard/)
